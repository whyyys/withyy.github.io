## Preface

At the end of 2022, the emergence of ChatGPT changed people's perception of artificial intelligence and brought about a phased transformation in the field of natural language processing (NLP). Large language models (LLMs) represented by the GPT series have become the mainstream of research in NLP and even the entire AI field. Since 2023, LLMs have been the core topic in the AI field, triggering wave after wave of technological tides.

LLMs are actually a derivative result of the classic research method in the NLP field, pre-trained language models (PLMs). The NLP field focuses on the processing, understanding, and generation of natural language texts written by humans. Since its birth, it has undergone multiple transformations: the symbolic stage, the statistical learning stage, the deep learning stage, the pre-trained model stage, and now the large model stage. PLMs represented by GPT and BERT are the core research achievements in the previous stage of the NLP field. With attention mechanism as the model architecture, through the phased idea of pre-training and fine-tuning, they achieve powerful natural language understanding capabilities by performing self-supervised pre-training on massive unsupervised texts. However, traditional PLMs still rely on a certain amount of supervised data for downstream task fine-tuning, and their performance on natural language generation tasks is not satisfactory. The performance of NLP systems is still far from the expected general artificial intelligence.

LLMs are breakthrough achievements based on PLMs, achieved by greatly expanding model parameters, pre-training data scale, and introducing instruction fine-tuning, human feedback reinforcement learning, and other means. Compared to traditional PLMs, LLMs have emergent abilities, with strong contextual learning capabilities, instruction understanding capabilities, and text generation capabilities. In the large model stage, NLP researchers can largely abandon the work of annotating large amounts of supervised data. By providing a small number of supervised examples, LLMs can achieve performance comparable to large-scale fine-tuned PLMs on specified downstream tasks. At the same time, powerful instruction understanding and text generation capabilities enable LLMs to directly, efficiently, and accurately respond to user instructions, thus truly approaching the goal of general artificial intelligence.

The breakthrough progress of LLMs has stimulated the research enthusiasm in the NLP field and even the entire AI field. Domestic and foreign universities, research institutes, major factories, and even many traditional field enterprises have invested in the wave of LLM research. Since 2023, stage-by-stage achievements in the LLM field have emerged one after another, with model performance constantly refreshing the upper limit. From the initial ChatGPT to GPT-4, and then to more powerful and customized models such as DeepSeek-R1 representing reasoning large models and Qwen-VL representing multimodal large models, LLM applications are also constantly emerging innovative applications that can enhance actual productivity and empower users' real lives. From the "hundred model battle" to the "Agent year", the research on LLM base may have tended to a stable pattern, but LLM research is always thriving. It can be affirmed that in the not-too-distant future, LLMs and applications based on LLMs will certainly become infrastructure in people's lives, inseparable from everyone's life, learning, and work.

In this context, it is crucial for every NLP researcher and even AI researchers in other fields to deeply understand and master the principles of LLMs, and to be able to apply and train any LLM hands-on. At the end of 2023, we respectively created two original open-source large model tutorials: self-llm [Open Source Large Model Guide:] (https://github.com/datawhalechina/self-llm) and llm-universe (Hands-on Learning Large Model Application Development: https://github.com/datawhalechina/llm-universe). The former aims to provide developers with a one-stop tutorial for deploying, inferring, and fine-tuning open-source LLMs, while the latter aims to guide developers to build their own LLM applications from scratch. Both tutorials have helped a wide range of domestic and foreign developers and received support and recognition from many developers. In the feedback from learners, we found that there is still a lack of a complete tutorial that explains LLM principles from scratch and guides learners to build and train LLMs hands-on.

In view of this, we have compiled this tutorial that combines LLM principles and practice. This book will start from the basic research methods of NLP, delve into the ideas and principles of LLMs layer by layer, and analyze the architectural foundation and training process of LLMs for readers in sequence. At the same time, we will combine the most mainstream code frameworks in the current LLM field to demonstrate how to build and train an LLM hands-on, in order to achieve both giving fish and teaching fishing. We hope that readers can start from this book to enter the vast world of LLMs and explore the infinite possibilities of LLMs.